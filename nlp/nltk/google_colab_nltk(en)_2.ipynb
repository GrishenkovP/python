{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google_colab_nltk(en)_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## По материалам курса \"Вводный курс ML\" (Дмитрий Макаров). Тема \"Обработка естественного языка\""
      ],
      "metadata": {
        "id": "33OID0F3LSYM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ttxaGCNV3kN9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'When we were in Paris we visited a lot of museums. We first went to the Louvre, the largest art museum in the world. I have always been interested in art so I spent many hours there. The museum is enourmous, so a week there would not be enough.'"
      ],
      "metadata": {
        "id": "cMQjqGGFDb6v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Разделение на предложения"
      ],
      "metadata": {
        "id": "LHF1zLqmDtf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "jpbTj5mhDz71"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "sentences = sent_tokenize(corpus)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXuxnOJYDnc_",
        "outputId": "f382ad6d-eee4-40c4-bfbe-46df3e10604e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['When we were in Paris we visited a lot of museums.', 'We first went to the Louvre, the largest art museum in the world.', 'I have always been interested in art so I spent many hours there.', 'The museum is enourmous, so a week there would not be enough.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Разделение на слова"
      ],
      "metadata": {
        "id": "MhBvT83TD_lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "LsTjgdQBEBQW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(sentences[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbUHEoxUEC9-",
        "outputId": "e824959c-342d-4f18-a6ea-4e71c336085f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'we', 'were', 'in', 'Paris', 'we', 'visited', 'a', 'lot', 'of', 'museums', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = []\n",
        " \n",
        "for sentence in sentences:\n",
        "    t = word_tokenize(sentence)\n",
        "    tokens.extend(t)\n",
        " \n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU-etRhXEunm",
        "outputId": "ca9e0391-7364-42d1-9b73-64f64feb50ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When', 'we', 'were', 'in', 'Paris', 'we', 'visited', 'a', 'lot', 'of', 'museums', '.', 'We', 'first', 'went', 'to', 'the', 'Louvre', ',', 'the', 'largest', 'art', 'museum', 'in', 'the', 'world', '.', 'I', 'have', 'always', 'been', 'interested', 'in', 'art', 'so', 'I', 'spent', 'many', 'hours', 'there', '.', 'The', 'museum', 'is', 'enourmous', ',', 'so', 'a', 'week', 'there', 'would', 'not', 'be', 'enough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Перевод в нижний регистр, удаление стоп-слов и знаков пунктуации"
      ],
      "metadata": {
        "id": "H9hvKmXpE1p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "oy3uhGmME3q-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "unique_stops = set(stopwords.words('english'))\n",
        " \n",
        "no_stops = []\n",
        " \n",
        "for token in tokens:\n",
        "    token = token.lower()\n",
        "    if token not in unique_stops and token.isalpha():\n",
        "        no_stops.append(token)\n",
        " \n",
        "print(no_stops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxGVqNfKE5pG",
        "outputId": "eeda1185-c8e4-484a-a63c-570d7e77fa37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['paris', 'visited', 'lot', 'museums', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hours', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Лемматизация"
      ],
      "metadata": {
        "id": "srbxTh4aFHZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "olGubCJdFFln"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        "lemmatized = []\n",
        " \n",
        "for token in no_stops:\n",
        "    token = lemmatizer.lemmatize(token)\n",
        "    lemmatized.append(token)\n",
        "    \n",
        "print(lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZdB3DQcFN7H",
        "outputId": "6215e32c-1176-4434-9430-5b5332942334"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['paris', 'visited', 'lot', 'museum', 'first', 'went', 'louvre', 'largest', 'art', 'museum', 'world', 'always', 'interested', 'art', 'spent', 'many', 'hour', 'museum', 'enourmous', 'week', 'would', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Стемминг"
      ],
      "metadata": {
        "id": "2RXJ0UO0FYa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "elq2tF65FWxG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "stemmed_p = [porter.stem(s) for s in lemmatized]\n",
        "print(stemmed_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Eiyi2vFg0F",
        "outputId": "3abaafb7-0632-4155-878c-758cbabe63f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pari', 'visit', 'lot', 'museum', 'first', 'went', 'louvr', 'largest', 'art', 'museum', 'world', 'alway', 'interest', 'art', 'spent', 'mani', 'hour', 'museum', 'enourm', 'week', 'would', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "cJBtRlvCFk5j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "stemmed_l = [lancaster.stem(s) for s in lemmatized]\n",
        "print(stemmed_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szgc3v-NFnC7",
        "outputId": "c8080804-3586-4016-9926-67c032775042"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['par', 'visit', 'lot', 'muse', 'first', 'went', 'louvr', 'largest', 'art', 'muse', 'world', 'alway', 'interest', 'art', 'spent', 'many', 'hour', 'muse', 'enourm', 'week', 'would', 'enough']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Мешок слов (bag of words, bow)"
      ],
      "metadata": {
        "id": "P3uUunMQGNTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "SbgAfkcZGSq9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_counter = Counter(lemmatized)\n",
        "print(bow_counter.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceZzY5uKGPEN",
        "outputId": "d56c007b-d08e-4989-8ee5-b53ced7ddf83"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('museum', 3), ('art', 2), ('paris', 1), ('visited', 1), ('lot', 1), ('first', 1), ('went', 1), ('louvre', 1), ('largest', 1), ('world', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "zRBC0pyLGY_N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer = \"word\", \n",
        "                             lowercase = True, \n",
        "                             tokenizer = None, \n",
        "                             preprocessor = None, \n",
        "                             stop_words = {'english'}, \n",
        "                             max_features = 5000)"
      ],
      "metadata": {
        "id": "1SGbkkgDGZ3F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_cv = vectorizer.fit_transform(sentences)"
      ],
      "metadata": {
        "id": "YwKeZip_Gb8V"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(bow_cv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA6FbzyIGf0S",
        "outputId": "ed357407-7705-4b52-fc72-ee144addebae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_cv.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt-dzyqZGjXK",
        "outputId": "c925a29f-592f-4991-ba16-7f4d7233fdbe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 2 0 0 1 1 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 3 0 1 0 1 0 1 0 0 1 0]\n",
            " [1 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorizer.vocabulary_\n",
        "print(vocab)\n",
        " \n",
        "tokens = vectorizer.get_feature_names_out()\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZUwEiu0Grui",
        "outputId": "79917d3f-24b9-4f4e-9ae8-ecb5dd52b47e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'when': 31, 'we': 27, 'were': 30, 'in': 9, 'paris': 20, 'visited': 26, 'lot': 13, 'of': 19, 'museums': 17, 'first': 6, 'went': 29, 'to': 25, 'the': 23, 'louvre': 14, 'largest': 12, 'art': 1, 'museum': 16, 'world': 32, 'have': 7, 'always': 0, 'been': 3, 'interested': 10, 'so': 21, 'spent': 22, 'many': 15, 'hours': 8, 'there': 24, 'is': 11, 'enourmous': 5, 'week': 28, 'would': 33, 'not': 18, 'be': 2, 'enough': 4}\n",
            "['always' 'art' 'be' 'been' 'enough' 'enourmous' 'first' 'have' 'hours'\n",
            " 'in' 'interested' 'is' 'largest' 'lot' 'louvre' 'many' 'museum' 'museums'\n",
            " 'not' 'of' 'paris' 'so' 'spent' 'the' 'there' 'to' 'visited' 'we' 'week'\n",
            " 'went' 'were' 'when' 'world' 'would']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_list = []\n",
        " \n",
        "for i, _ in enumerate(bow_cv):\n",
        " \n",
        "    index_list.append(f'Sentence_{i}')\n",
        " \n",
        "bow_cv_df = pd.DataFrame(data = bow_cv.toarray(), \n",
        "                         index = index_list, \n",
        "                         columns = tokens)"
      ],
      "metadata": {
        "id": "Jhm98ixhGxi0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_cv_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2agMPk7G4_8",
        "outputId": "75a6059e-2f2a-4258-c306-3fb0e1604dc9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            always  art  be  been  enough  ...  went  were  when  world  would\n",
            "Sentence_0       0    0   0     0       0  ...     0     1     1      0      0\n",
            "Sentence_1       0    1   0     0       0  ...     1     0     0      1      0\n",
            "Sentence_2       1    1   0     1       0  ...     0     0     0      0      0\n",
            "Sentence_3       0    0   1     0       1  ...     0     0     0      0      1\n",
            "\n",
            "[4 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "A2dh0w7UHSB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из Википедии \"Если документ содержит 100 слов, и слово[3] «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,03 (3/100). Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству документов, содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 документах из 10 000 000 документов, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,03 × 4 = 0,12\""
      ],
      "metadata": {
        "id": "dWC6QHdyIdsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логика формулы следующая, чем выше частота слова в документе (tf) и чем реже оно встречается в целом в документах (idf), тем выше общий показатель (tf-idf)."
      ],
      "metadata": {
        "id": "SLMVHxfPIiNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### IDF"
      ],
      "metadata": {
        "id": "70tIBD5nJRZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "uq8vG6w5HTPs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_trans = TfidfTransformer(smooth_idf = True, use_idf = True)"
      ],
      "metadata": {
        "id": "-WLbYIFgI9ZD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_trans.fit(bow_cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedaFgeNI_Hb",
        "outputId": "dbce9947-8bef-41ef-dbe9-05c37678b5c2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_idf = pd.DataFrame(tfidf_trans.idf_, index = tokens, columns = [\"idf_weights\"])"
      ],
      "metadata": {
        "id": "RbzXBnP6JC48"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKbihgKNJmuz",
        "outputId": "e272d8b7-6170-4db2-a2a9-cde18efa1b30"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            idf_weights\n",
            "always         1.916291\n",
            "art            1.510826\n",
            "be             1.916291\n",
            "been           1.916291\n",
            "enough         1.916291\n",
            "enourmous      1.916291\n",
            "first          1.916291\n",
            "have           1.916291\n",
            "hours          1.916291\n",
            "in             1.223144\n",
            "interested     1.916291\n",
            "is             1.916291\n",
            "largest        1.916291\n",
            "lot            1.916291\n",
            "louvre         1.916291\n",
            "many           1.916291\n",
            "museum         1.510826\n",
            "museums        1.916291\n",
            "not            1.916291\n",
            "of             1.916291\n",
            "paris          1.916291\n",
            "so             1.510826\n",
            "spent          1.916291\n",
            "the            1.510826\n",
            "there          1.510826\n",
            "to             1.916291\n",
            "visited        1.916291\n",
            "we             1.510826\n",
            "week           1.916291\n",
            "went           1.916291\n",
            "were           1.916291\n",
            "when           1.916291\n",
            "world          1.916291\n",
            "would          1.916291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF x IDF"
      ],
      "metadata": {
        "id": "m4rmVlu-JTvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_vector = tfidf_trans.transform(bow_cv)\n",
        "tf_idf_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC0lRw4yJD48",
        "outputId": "ee4ee75d-9ccd-4532-9870-1601d972a4ef"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x34 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 42 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_tfidf = pd.DataFrame(tf_idf_vector.toarray(), columns = vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "7XzQddayJY-r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_tfidf.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGuKVnsFJa77",
        "outputId": "d0a62e4b-4614-4669-8726-2220142e449b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   0         1         2         3\n",
            "always      0.000000  0.000000  0.328404  0.000000\n",
            "art         0.000000  0.211724  0.258918  0.000000\n",
            "be          0.000000  0.000000  0.000000  0.324676\n",
            "been        0.000000  0.000000  0.328404  0.000000\n",
            "enough      0.000000  0.000000  0.000000  0.324676\n",
            "enourmous   0.000000  0.000000  0.000000  0.324676\n",
            "first       0.000000  0.268544  0.000000  0.000000\n",
            "have        0.000000  0.000000  0.328404  0.000000\n",
            "hours       0.000000  0.000000  0.328404  0.000000\n",
            "in          0.202925  0.171408  0.209616  0.000000\n",
            "interested  0.000000  0.000000  0.328404  0.000000\n",
            "is          0.000000  0.000000  0.000000  0.324676\n",
            "largest     0.000000  0.268544  0.000000  0.000000\n",
            "lot         0.317921  0.000000  0.000000  0.000000\n",
            "louvre      0.000000  0.268544  0.000000  0.000000\n",
            "many        0.000000  0.000000  0.328404  0.000000\n",
            "museum      0.000000  0.211724  0.000000  0.255978\n",
            "museums     0.317921  0.000000  0.000000  0.000000\n",
            "not         0.000000  0.000000  0.000000  0.324676\n",
            "of          0.317921  0.000000  0.000000  0.000000\n",
            "paris       0.317921  0.000000  0.000000  0.000000\n",
            "so          0.000000  0.000000  0.258918  0.255978\n",
            "spent       0.000000  0.000000  0.328404  0.000000\n",
            "the         0.000000  0.635171  0.000000  0.255978\n",
            "there       0.000000  0.000000  0.258918  0.255978\n",
            "to          0.000000  0.268544  0.000000  0.000000\n",
            "visited     0.317921  0.000000  0.000000  0.000000\n",
            "we          0.501305  0.211724  0.000000  0.000000\n",
            "week        0.000000  0.000000  0.000000  0.324676\n",
            "went        0.000000  0.268544  0.000000  0.000000\n",
            "were        0.317921  0.000000  0.000000  0.000000\n",
            "when        0.317921  0.000000  0.000000  0.000000\n",
            "world       0.000000  0.268544  0.000000  0.000000\n",
            "would       0.000000  0.000000  0.000000  0.324676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "nHIHs6mYJssN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfIdfVectorizer = TfidfVectorizer(use_idf = True, stop_words= 'english')"
      ],
      "metadata": {
        "id": "y7qTXl1-JuEj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfIdf = tfIdfVectorizer.fit_transform(sentences)\n",
        "tfIdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swK-bJvgJwoL",
        "outputId": "062e12cf-7085-41ba-9556-95a85b422345"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x15 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfIdfVectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtG3ZFAcJ0Pz",
        "outputId": "625759f5-1760-4505-fe14-757037195720"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['art' 'enourmous' 'hours' 'interested' 'largest' 'lot' 'louvre' 'museum'\n",
            " 'museums' 'paris' 'spent' 'visited' 'week' 'went' 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfIdfVectorizer.idf_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWoEjAUIJ2qD",
        "outputId": "8b26b056-e452-42f9-ae3c-5d027963ba32"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.51082562, 1.91629073, 1.91629073, 1.91629073, 1.91629073,\n",
              "       1.91629073, 1.91629073, 1.51082562, 1.91629073, 1.91629073,\n",
              "       1.91629073, 1.91629073, 1.91629073, 1.91629073, 1.91629073])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_idf = pd.DataFrame(tfIdfVectorizer.idf_, index = tfIdfVectorizer.get_feature_names_out(), columns = ['idf_weights'])"
      ],
      "metadata": {
        "id": "LBqZzaSCJ-w7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrZ-6XpkJ_qL",
        "outputId": "54acc333-720c-4097-cfa1-db1b59d05889"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            idf_weights\n",
            "art            1.510826\n",
            "enourmous      1.916291\n",
            "hours          1.916291\n",
            "interested     1.916291\n",
            "largest        1.916291\n",
            "lot            1.916291\n",
            "louvre         1.916291\n",
            "museum         1.510826\n",
            "museums        1.916291\n",
            "paris          1.916291\n",
            "spent          1.916291\n",
            "visited        1.916291\n",
            "week           1.916291\n",
            "went           1.916291\n",
            "world          1.916291\n"
          ]
        }
      ]
    }
  ]
}